# YOLOv8 Video Object Detection & Tracking

## ğŸ“š Projektbeschreibung
Ziel des Projektes ist zu zeigen, wie sich eine Maschinenstillstandsanalyse in wenigen Schritten durchfÃ¼hren lÃ¤sst.
Dieses Repository zeigt, wie man mit **YOLOv8** Objekte in Videos erkennt, die Positionen speichert und Bewegungsanalysen durchfÃ¼hrt. 

## ğŸ“Œ Beispiel fÃ¼r die Objekterkennung  

Das folgende Bild zeigt ein Beispiel aus dem Video, in dem der **Roboter das Werkzeug und den FrÃ¤ser** erkennt:  

![Beispiel fÃ¼r die Objekterkennung](val_batch1_labels.jpg)  

Falls das Bild in einem Unterordner (`images/`) liegt, Ã¤ndere den Pfad entsprechend:  

```markdown
![Beispiel fÃ¼r die Objekterkennung](images/val_batch1_labels.jpg)



## ğŸ“š Funktionen
- ğŸ¥ **Objekterkennung in Videos mit YOLOv8**
- ğŸ“Š **Speicherung der Objektpositionen als CSV**
- ğŸ“Š **Visualisierung der X- und Y-Positionen Ã¼ber die Zeit**
- â° **Stillstandsanalyse mit Vektoren**
- ğŸŒ **Live-Analyse mit OpenCV**


## ğŸ“Œ Installationsanleitung & AbhÃ¤ngigkeiten  

### ğŸ”¹ Python-Version  
Dieses Projekt wurde mit **Python 3.11.11** getestet.  
Es wird empfohlen, eine **virtuelle Umgebung** zu verwenden, um Konflikte mit bestehenden Paketen zu vermeiden.  

---

### ğŸ”¹ BenÃ¶tigte AbhÃ¤ngigkeiten  
Folgende Python-Pakete sind fÃ¼r das Projekt erforderlich:  
- `ultralytics` â†’ YOLOv8 fÃ¼r Objekterkennung & Training  
- `opencv-python` â†’ Verarbeitung & Anzeige von Videos & Bildern  
- `pandas` â†’ Speicherung & Analyse der erkannten Objektdaten  
- `matplotlib` â†’ Visualisierung der X- & Y-Positionen  
- `numpy` â†’ Berechnung der Stillstandsvektoren  

Diese sind in der Datei **`requirements.txt`** hinterlegt und kÃ¶nnen mit folgendem Befehl installiert werden:  

```bash
pip install -r requirements.txt
```
---

## ğŸ”§ Vorgehensweise

### **Schritt 1:** Video der Aufgabe erstellen oder herunterladen.
### **Schritt 2:** Frames des Videos extrahieren und Bilder in einem Ordner speichern.
### **Schritt 3:** Labelme Ã¼ber `pip` installieren:
```bash
pip install labelme
```

### **Schritt 4:** Objekte in den Frames mit Labelme labeln und als JSON speichern (mindestens 100 Bilder fÃ¼r gute Erkennungsrate).

### **Schritt 5:** JSON-Labelinformationen in YOLO TXT-Format umwandeln und Bilder in Training & Validierung aufteilen.

### **Schritt 6:** YOLO Ã¼ber `pip` installieren:
```bash
pip install ultralytics
```

### **Schritt 7:** YOLO-Training durchfÃ¼hren:
```bash
python scripts/train.py --epochs 50 --data dataset/data.yaml
```

### **Schritt 8:** Bewertung der Ergebnisse und Testen mit unannotierten Bildern.

### **Schritt 9:** Test des YOLO-Modells auf einem Live-Video der gleichen Aufgabe:
```bash
python scripts/video_detection.py --video data/sample.mp4
```

### **Schritt 10:** ErgÃ¤nzung der Datenerfassung fÃ¼r erkannte Objekte (Zeitstempel, Position X/Y) und Speicherung in JSON/CSV.

### **Schritt 11:** Auswertung der Daten zur ÃœberprÃ¼fung der Objektbewegung und Stillstandsanalysen.

---

## ğŸ› ï¸ Installation
### 1. **Python & AbhÃ¤ngigkeiten installieren**
```bash
pip install -r requirements.txt
```

### 2. **Repo klonen** (Falls du Git nutzt)
```bash
git clone https://github.com/dein-username/yolo-video-detection.git
cd yolo-video-detection
```

---

## ğŸ¥ Verwendung
### **1. YOLO-Training (optional)**
Falls du dein eigenes Modell trainieren willst, nutze folgendes Kommando:
```bash
python scripts/train.py --epochs 50 --data dataset/data.yaml
```

### **2. Live-Objekterkennung in Videos**
```bash
python scripts/video_detection.py --video data/sample.mp4
```

### **3. Positions- & Bewegungsanalyse speichern**
```bash
python scripts/analyze_positions.py --csv results/object_detections.csv
```

---

## ğŸ† Beispielcode: Objekterkennung in Videos
```python
import cv2
from ultralytics import YOLO

model = YOLO("models/best.pt")
cap = cv2.VideoCapture("data/sample.mp4")
while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break
    results = model(frame)
    for result in results:
        for box in result.boxes:
            x1, y1, x2, y2 = map(int, box.xyxy[0])
            label = model.names[int(box.cls[0])]
            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
            cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
    cv2.imshow("YOLO Live Detection", frame)
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break
cap.release()
cv2.destroyAllWindows()
```

---

## ğŸ“ˆ Beispiel: Stillstandsanalyse
```python
python scripts/analyze_stillstand.py --csv results/object_detections.csv --threshold 10 --frame-step 5
```

---

## ğŸ› ï¸ To-Do
- [ ] Weitere Visualisierungen hinzufÃ¼gen
- [ ] Webinterface mit Streamlit implementieren
- [ ] Erweiterung auf Echtzeit-Webcam-Daten

---

## ğŸ† Autor
**Hubert WÃ¼rschinger**  
LinkedIn: [Hubert WÃ¼rschinger](https://www.linkedin.com/in/hubert-w%C3%BCrschinger-82031813b/)  
GitHub: [Hubert Wuerschinger](https://github.com/HubertWuerschinger)

---

## âœ¨ Lizenz
Dieses Projekt steht unter der **MIT-Lizenz**. FÃ¼hle dich frei, es zu nutzen und anzupassen! ğŸš€

